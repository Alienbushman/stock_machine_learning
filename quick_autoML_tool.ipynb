{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dylan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/dylan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/dylan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using linear regression\n",
      "the accuracy is 0.6151545363908275\n",
      "the balanced accuracy is 0.6130615475155784\n",
      "{'white', 'black'}\n",
      "{'white', 'black', 'draw'}\n",
      "When using Extra tree classifier\n",
      "the accuracy is 0.6331006979062812\n",
      "the balanced accuracy is 0.6559586528394061\n",
      "{'white', 'black', 'draw'}\n",
      "{'white', 'black', 'draw'}\n",
      "When using support vector classifier\n",
      "the accuracy is 0.6151545363908275\n",
      "the balanced accuracy is 0.6129740915017858\n",
      "{'white', 'black'}\n",
      "{'white', 'black', 'draw'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dylan/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using random forest classifier\n",
      "the accuracy is 0.6321036889332003\n",
      "the balanced accuracy is 0.6860650705954573\n",
      "{'white', 'black', 'draw'}\n",
      "{'white', 'black', 'draw'}\n",
      "When using gradient boosted classifier\n",
      "the accuracy is 0.6370887337986042\n",
      "the balanced accuracy is 0.4247419791715313\n",
      "{'white', 'black', 'draw'}\n",
      "{'white', 'black', 'draw'}\n",
      "When using XGBoost\n",
      "the accuracy is 0.6545363908275175\n",
      "the balanced accuracy is 0.7685912368164006\n",
      "{'white', 'black', 'draw'}\n",
      "{'white', 'black', 'draw'}\n",
      "When using catboost classifier\n",
      "the accuracy is 0.8095712861415753\n",
      "the balanced accuracy is 0.5395501923203859\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a30d2344dce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_feature_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'winner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mrun_generic_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-a30d2344dce0>\u001b[0m in \u001b[0;36mrun_generic_models\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'When using '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "def keep_relavent_columns(df, column_names=None):\n",
    "    if column_names is None:\n",
    "        return df\n",
    "    return df[column_names]\n",
    "\n",
    "def encode_one_hot(df):\n",
    "    columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
    "    for col in columnsToEncode:\n",
    "        if len(df[col].unique()) < 50:\n",
    "            df = pd.concat([df,pd.get_dummies(df[col], prefix=[col])], axis=1)\n",
    "        df.drop(col,inplace=True,axis=1)\n",
    "    return df\n",
    "\n",
    "def normalize_data(df):\n",
    "    columnsToEncode = list(df.select_dtypes(include=['float','int']))\n",
    "    for col in columnsToEncode:\n",
    "        df[col]=(df[col]-df[col].mean())/df[col].std()\n",
    "    return df\n",
    "\n",
    "def apply_to_numberic_selective(df):\n",
    "    columnsToEncode = list(df.select_dtypes(include=['float','int']))\n",
    "    for col in columnsToEncode:\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "def process_column_names_xgboost(df):\n",
    "    regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "    df.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df.columns.values]\n",
    "    return df\n",
    "\n",
    "def preprocessing(df, target, column_names=None,bad_columns=None, apply_onehot=True, using_xgboost=True):\n",
    "    # Avoids processing target feature\n",
    "    target_df=df[target]\n",
    "    df.drop(target, inplace=True, axis=1)\n",
    "    \n",
    "    df = keep_relavent_columns(df,column_names)\n",
    "    df = drop_bad_columns(df, bad_columns)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    df = normalize_data(df)\n",
    "    if apply_onehot:\n",
    "        df = encode_one_hot(df)\n",
    "        df = df.astype('float64')\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    else :\n",
    "        df = apply_to_numberic_selective(df)\n",
    "    if using_xgboost:\n",
    "        df = process_column_names_xgboost(df)\n",
    "    #reads the target feature after processing\n",
    "    df_merged = df.merge(target_df, how='inner', left_index=True, right_index=True)\n",
    "    return df_merged\n",
    "    \n",
    "def label_feature_split(df, column):\n",
    "    label=df[[column]].values.ravel()\n",
    "    feature=df.drop([column], axis=1)\n",
    "    return feature, label\n",
    "\n",
    "def split_dataset(df):\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train, validation = train_test_split(train, test_size=0.125)\n",
    "    return train, validation, test\n",
    "\n",
    "def drop_bad_columns(df, columns=None):\n",
    "    if columns is not None:\n",
    "        return df.drop(columns, axis=1)\n",
    "    return df\n",
    "\n",
    "#This can easily be extended for other metrics, especially for binary labels\n",
    "def metrics(y_pred, y_test):\n",
    "    from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "    print('the accuracy is '+str(accuracy_score(y_pred, y_test)))\n",
    "    print('the balanced accuracy is '+str(balanced_accuracy_score(y_pred, y_test)))   \n",
    "    \n",
    "def run_generic_models(X_train, y_train, X_test, y_test):\n",
    "    #Using the recomended classifiers\n",
    "    #https://arxiv.org/abs/1708.05070\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from catboost import CatBoostClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "    RFC = RandomForestClassifier(n_estimators=500, max_features=0.25, criterion=\"entropy\")\n",
    "    SVM = SVC(C = 0.01, gamma=0.1, kernel=\"poly\", degree=3, coef0=10.0)\n",
    "    ETC = ExtraTreesClassifier(n_estimators=1000, max_features=\"log2\", criterion=\"entropy\")\n",
    "    LR = LogisticRegression(C=1.5, penalty=\"l1\",fit_intercept=True)\n",
    "    # Models that were not included in the paper not from SKlearn\n",
    "    XGC = XGBClassifier()\n",
    "    CBC = CatBoostClassifier(silent=True)\n",
    "    \n",
    "    models=[(LR, \"linear regression\"),(ETC, \"Extra tree classifier\"),(SVM, \"support vector classifier\"), (RFC, \"random forest classifier\"), (GBC, \"gradient boosted classifier\"),\n",
    "             (XGC, \"XGBoost\"), (CBC, \"catboost classifier\")]\n",
    "    for model, name in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print('When using '+ name)\n",
    "        metrics(y_pred,y_test)\n",
    "    \n",
    "df = pd.read_csv('Datasets/games.csv')\n",
    "processed_features_df = preprocessing(df, 'winner', bad_columns='victory_status')\n",
    "\n",
    "train_df, validation_df, test_df = split_dataset(processed_features_df)\n",
    "X_train, y_train = label_feature_split(train_df,'winner')\n",
    "X_validation, y_validation = label_feature_split(validation_df, 'winner')\n",
    "X_test, y_test = label_feature_split(test_df, 'winner')\n",
    "\n",
    "run_generic_models(X_train, y_train, X_validation, y_validation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
