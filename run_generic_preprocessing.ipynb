{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/rick/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using linear regression\n",
      "the accuracy is 0.6186440677966102\n",
      "the balanced accuracy is 0.578276652984331\n",
      "When using Extra tree classifier\n",
      "the accuracy is 0.6390827517447657\n",
      "the balanced accuracy is 0.6591723954935763\n",
      "When using support vector classifier\n",
      "the accuracy is 0.6191425722831505\n",
      "the balanced accuracy is 0.618842644204963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using random forest classifier\n",
      "the accuracy is 0.6430707876370887\n",
      "the balanced accuracy is 0.696595096545645\n",
      "When using gradient boosted classifier\n",
      "the accuracy is 0.6430707876370887\n",
      "the balanced accuracy is 0.5247063774671137\n",
      "When using XGBoost\n",
      "the accuracy is 0.6779661016949152\n",
      "the balanced accuracy is 0.6181611140292141\n",
      "When using Light GBM\n",
      "the accuracy is 0.8295114656031904\n",
      "the balanced accuracy is 0.7865851251123009\n",
      "When using catboost classifier\n",
      "the accuracy is 0.8085742771684945\n",
      "the balanced accuracy is 0.5392100024447316\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "def keep_relavent_columns(df, column_names=None):\n",
    "    if column_names is None:\n",
    "        return df\n",
    "    return df[column_names]\n",
    "\n",
    "def encode_one_hot(df):\n",
    "    columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
    "    for col in columnsToEncode:\n",
    "        if len(df[col].unique()) < 50:\n",
    "            df = pd.concat([df,pd.get_dummies(df[col], prefix=[col])], axis=1)\n",
    "        df.drop(col,inplace=True,axis=1)\n",
    "    return df\n",
    "\n",
    "def normalize_data(df):\n",
    "    columnsToEncode = list(df.select_dtypes(include=['float','int']))\n",
    "    for col in columnsToEncode:\n",
    "        df[col]=(df[col]-df[col].mean())/df[col].std()\n",
    "    return df\n",
    "\n",
    "def apply_to_numberic_selective(df):\n",
    "    columnsToEncode = list(df.select_dtypes(include=['float','int']))\n",
    "    for col in columnsToEncode:\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "def process_column_names_xgboost(df):\n",
    "    regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "    df.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df.columns.values]\n",
    "    return df\n",
    "\n",
    "def preprocessing(df, target, column_names=None,bad_columns=None, apply_onehot=True, using_xgboost=True):\n",
    "    # Avoids processing target feature\n",
    "    target_df=df[target]\n",
    "    df.drop(target, inplace=True, axis=1)\n",
    "    \n",
    "    df = keep_relavent_columns(df,column_names)\n",
    "    df = drop_bad_columns(df, bad_columns)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    df = normalize_data(df)\n",
    "    if apply_onehot:\n",
    "        df = encode_one_hot(df)\n",
    "        df = df.astype('float64')\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    else :\n",
    "        df = apply_to_numberic_selective(df)\n",
    "    if using_xgboost:\n",
    "        df = process_column_names_xgboost(df)\n",
    "    #reads the target feature after processing\n",
    "    df_merged = df.merge(target_df, how='inner', left_index=True, right_index=True)\n",
    "    return df_merged\n",
    "    \n",
    "def label_feature_split(df, column):\n",
    "    label=df[[column]].values.ravel()\n",
    "    feature=df.drop([column], axis=1)\n",
    "    return feature, label\n",
    "\n",
    "def split_dataset(df):\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train, validation = train_test_split(train, test_size=0.125)\n",
    "    return train, validation, test\n",
    "\n",
    "def drop_bad_columns(df, columns=None):\n",
    "    if columns is not None:\n",
    "        return df.drop(columns, axis=1)\n",
    "    return df\n",
    "\n",
    "#This can easily be extended for other metrics, especially for binary labels\n",
    "def metrics(y_pred, y_test):\n",
    "    from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "    print('the accuracy is '+str(accuracy_score(y_pred, y_test)))\n",
    "    print('the balanced accuracy is '+str(balanced_accuracy_score(y_pred, y_test)))   \n",
    "    \n",
    "def run_generic_models(X_train, y_train, X_test, y_test):\n",
    "    #Using the recomended classifiers\n",
    "    #https://arxiv.org/abs/1708.05070\n",
    "    \n",
    "    GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "    RFC = RandomForestClassifier(n_estimators=500, max_features=0.25, criterion=\"entropy\")\n",
    "    SVM = SVC(C = 0.01, gamma=0.1, kernel=\"poly\", degree=3, coef0=10.0)\n",
    "    ETC = ExtraTreesClassifier(n_estimators=1000, max_features=\"log2\", criterion=\"entropy\")\n",
    "    LR = LogisticRegression(C=1.5, penalty=\"l1\",fit_intercept=True)\n",
    "    # Models that were not included in the paper not from SKlearn\n",
    "    XGC = XGBClassifier()\n",
    "    CBC = CatBoostClassifier(silent=True)\n",
    "    light_gb = lgb.LGBMClassifier()\n",
    "    \n",
    "    models=[(LR, \"linear regression\"),(ETC, \"Extra tree classifier\"),(SVM, \"support vector classifier\"), (RFC, \"random forest classifier\"), (GBC, \"gradient boosted classifier\"),\n",
    "             (XGC, \"XGBoost\"),(light_gb,\"Light GBM\"), (CBC, \"catboost classifier\")]\n",
    "    #models=[()]\n",
    "    for model, name in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print('When using '+ name)\n",
    "        metrics(y_pred,y_test)\n",
    "    \n",
    "df = pd.read_csv('Datasets/games.csv')\n",
    "target_feature ='winner'\n",
    "\n",
    "processed_features_df = preprocessing(df, target_feature, bad_columns='victory_status')\n",
    "\n",
    "train_df, validation_df, test_df = split_dataset(processed_features_df)\n",
    "X_train, y_train = label_feature_split(train_df,target_feature)\n",
    "X_validation, y_validation = label_feature_split(validation_df, target_feature)\n",
    "X_test, y_test = label_feature_split(test_df, target_feature)\n",
    "\n",
    "run_generic_models(X_train, y_train, X_validation, y_validation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
